{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEH9tjhSCGSv",
        "outputId": "813ee4c6-14d1-47ea-c252-d57fee4f5da2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        " !pip install -q git+https://github.com/tensorflow/docs@r1.14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsheWkcTx1iL",
        "outputId": "c6f6c0f6-6d22-4d2d-8a36-88c4706d8966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow==2.9.2\n",
            "tensorflow-datasets==4.6.0\n",
            "tensorflow-docs @ git+https://github.com/tensorflow/docs@d1b9b9f90e6203ce4f3f9334045deede3aa7d9a1\n",
            "tensorflow-estimator==2.9.0\n",
            "tensorflow-gcs-config==2.9.1\n",
            "tensorflow-hub==0.12.0\n",
            "tensorflow-io-gcs-filesystem==0.27.0\n",
            "tensorflow-metadata==1.11.0\n",
            "tensorflow-probability==0.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip freeze | grep tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "in2Bu6SO2UIX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "8bca99ef-34cc-4378-b814-2a5051bcd106"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f1d473c02fed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_docs.vis'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# from tensorflow_docs.vis import embed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dtwhAQhyQiC",
        "outputId": "c54f33e3-1c62-40bc-8eb0-f7353d638659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        " !pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bM5fGTuuCc5b"
      },
      "outputs": [],
      "source": [
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow import keras\n",
        "from imutils import paths\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o3QtBEUCSI4",
        "outputId": "c97b4590-ce34-49f0-e48b-307d1304333d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MZcdDDXOCut6"
      },
      "outputs": [],
      "source": [
        "#Define hyper parameters\n",
        "IMG_SIZE = 540 \n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 200\n",
        "\n",
        "MAX_SEQ_LENGTH = 500 # if number of frames is less than 500, fill with 0 (padding concept). \n",
        "NUM_FEATURES = 1536 # Number of features we want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bP7n0WMvRZZ",
        "outputId": "ba778a54-482f-4710-d519-0b4501b4bf35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['10', '5', '1', '3', '6', '8', '9', '4', '2', '7']\n",
            "['/content/drive/MyDrive/Dataset/dataset_4/10/350.avi', '/content/drive/MyDrive/Dataset/dataset_4/5/350.avi', '/content/drive/MyDrive/Dataset/dataset_4/1/350.avi', '/content/drive/MyDrive/Dataset/dataset_4/3/350.avi', '/content/drive/MyDrive/Dataset/dataset_4/6/350.avi', '/content/drive/MyDrive/Dataset/dataset_4/8/350.avi', '/content/drive/MyDrive/Dataset/dataset_4/9/350.avi', '/content/drive/MyDrive/Dataset/dataset_4/4/350.avi', '/content/drive/MyDrive/Dataset/dataset_4/2/350.avi', '/content/drive/MyDrive/Dataset/dataset_4/7/350.avi']\n",
            "  tag                                         video_name\n",
            "0  10  /content/drive/MyDrive/Dataset/dataset_4/10/35...\n",
            "1   5  /content/drive/MyDrive/Dataset/dataset_4/5/350...\n",
            "2   1  /content/drive/MyDrive/Dataset/dataset_4/1/350...\n",
            "3   3  /content/drive/MyDrive/Dataset/dataset_4/3/350...\n",
            "4   6  /content/drive/MyDrive/Dataset/dataset_4/6/350...\n",
            "5   8  /content/drive/MyDrive/Dataset/dataset_4/8/350...\n",
            "6   9  /content/drive/MyDrive/Dataset/dataset_4/9/350...\n",
            "7   4  /content/drive/MyDrive/Dataset/dataset_4/4/350...\n",
            "8   2  /content/drive/MyDrive/Dataset/dataset_4/2/350...\n",
            "9   7  /content/drive/MyDrive/Dataset/dataset_4/7/350...\n"
          ]
        }
      ],
      "source": [
        "data_path = '/content/drive/MyDrive/Dataset/dataset_4'\n",
        "tag = [] # classes\n",
        "video_name_phases = []\n",
        "for fol in os.listdir(data_path):\n",
        "  for video in os.listdir(os.path.join(data_path, fol)):\n",
        "    tag.append(fol)\n",
        "    video_name_phases.append(data_path + '/' + fol + '/' + video)\n",
        "print(tag)\n",
        "print(video_name_phases )\n",
        "df_train = pd.DataFrame({'tag': tag, 'video_name': video_name_phases })\n",
        "print(df_train) # show some data from data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6hUw8f4hCuxu"
      },
      "outputs": [],
      "source": [
        "# Here we have two functions\n",
        "def crop_center_square_xy(frame): # crop the center \n",
        "    y_shape, x_shape = frame.shape[0:2]\n",
        "    min_dim = min(y_shape, x_shape)\n",
        "    start_x_1 = (x_shape // 2) - (min_dim // 2)\n",
        "    start_y_1 = (y_shape // 2) - (min_dim // 2)\n",
        "    return frame[start_y_1 : start_y_1 + min_dim, start_x_1 : start_x_1 + min_dim]\n",
        "# Phases extraction\n",
        "# Now we have data ( with series of frames and classes. example, class 1, has  series of frames and so on.. ), Now we will  extract frames of each phases  for phase predictions.\n",
        "#'''this function to extract frame from each phase video output'''\n",
        "def load_video(path, max_frames=20):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square_xy(frame)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)\n",
        "# Once we are done  with frame then we will do feature extractions.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQwGgUV6Cu2V",
        "outputId": "ebeb9db1-1bcc-447b-98a9-9ab511aa4a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219055592/219055592 [==============================] - 8s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Now we we have frames of each phases ( It means images of frames of each phases)\n",
        "#Next step is to extract features using InceptionResNetV2 \n",
        "#https://keras.io/api/applications/densenet/ # pretrained model, Transfering model here\n",
        "def feature_extractor():\n",
        "    feature_extractor_build = keras.applications.InceptionResNetV2(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3), # Image Size with RGB\n",
        "    )\n",
        "    input_as_preprocess = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    input_value = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed_value = input_as_preprocess(input_value)\n",
        "\n",
        "    output_value = feature_extractor_build(preprocessed_value)\n",
        "    return keras.Model(input_value , output_value, name=\"feature_extractor\")\n",
        "\n",
        "# Now intializing the this   parameter\n",
        "feature_extractor = feature_extractor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQGMXfWWCu5f",
        "outputId": "af9f9470-d7c5-45b5-89d8-c22155c3b8ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1', '10', '2', '3', '4', '5', '6', '7', '8', '9']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/numeric.py:2446: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  return bool(asarray(a1 == a2).all())\n"
          ]
        }
      ],
      "source": [
        "# This is label Encoding, As our class is has string values and machine learning do not deal with this.\n",
        "# Label preprocessing with StringLookup.\n",
        "label_processor_phases = keras.layers.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(os.listdir(data_path))\n",
        ")\n",
        "print(label_processor_phases.get_vocabulary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DZtbmoaCu8z",
        "outputId": "6a9c5979-5b5c-484d-d5a0-3922ed6ff12a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 6s 6s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "Features of frames in train set: (10, 500, 1536)\n",
            " Features of Frame masks in train set: (10, 500)\n"
          ]
        }
      ],
      "source": [
        "# from here what we discussed like encoding, creatng frames and all that, i have created one functions and put all in this function \n",
        "# Putting all data together, like padding, add batch dimention, feature extractions.\n",
        "# Encapsulating\n",
        "def prepare_all_videos_all_phases(df):\n",
        "  total_number_samples = len(df)\n",
        "  video_paths = df[\"video_name\"].values.tolist()\n",
        "  labels_value = df[\"tag\"].values\n",
        "  labels = label_processor_phases(labels_value[..., None]).numpy()\n",
        "  frame_masks_value = np.zeros(shape=(total_number_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
        "  frame_features_value = np.zeros(\n",
        "      shape=(total_number_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "  )\n",
        "\n",
        "  # For every  video.\n",
        "  for idx, path in enumerate(video_paths):\n",
        "      total_frames_value = load_video(path) # Gather all  frames and also  add a batch dimension.\n",
        "      frames = total_frames_value[None, ...]\n",
        "\n",
        "      # placeholders for storing the masks and features of the current video.\n",
        "      temp_frame_mask_value = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "      temp_frame_features_value = np.zeros(\n",
        "          shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "      )\n",
        "\n",
        "      # Extracting all  features from the frames .\n",
        "      for k, batch in enumerate(frames):\n",
        "          video_length = batch.shape[0]\n",
        "          length = min(MAX_SEQ_LENGTH, video_length)\n",
        "          for j in range(length):\n",
        "              temp_frame_features_value[k, j, :] = feature_extractor.predict(\n",
        "                  batch[None, j, :]\n",
        "              )\n",
        "          temp_frame_mask_value[k, :length] = 1  \n",
        "\n",
        "      frame_features_value[idx,] = temp_frame_features_value.squeeze()\n",
        "      frame_masks_value[idx,] = temp_frame_mask_value.squeeze()\n",
        "\n",
        "  return (frame_features_value, frame_masks_value), labels\n",
        "\n",
        "\n",
        "# using this function for  Train set and test set\n",
        "test_data, test_labels = prepare_all_videos_all_phases(df_train)\n",
        "train_data, train_labels = prepare_all_videos_all_phases(df_train) \n",
        "\n",
        "\n",
        "print(f\"Features of frames in train set: {train_data[0].shape}\")\n",
        "print(f\" Features of Frame masks in train set: {train_data[1].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "j6Dp1oVXFX4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96a41cd-64b5-4591-b558-0f1865e52245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.3030 - accuracy: 0.1429\n",
            "Epoch 1: val_loss improved from inf to 2.51250, saving model to /tmp/video_classifier\n",
            "1/1 [==============================] - 6s 6s/step - loss: 2.3030 - accuracy: 0.1429 - val_loss: 2.5125 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2563 - accuracy: 0.1429\n",
            "Epoch 2: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2563 - accuracy: 0.1429 - val_loss: 3.3687 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0964 - accuracy: 0.1429\n",
            "Epoch 3: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0964 - accuracy: 0.1429 - val_loss: 5.7964 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1498 - accuracy: 0.1429\n",
            "Epoch 4: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1498 - accuracy: 0.1429 - val_loss: 2.6655 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1180 - accuracy: 0.1429\n",
            "Epoch 5: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.1180 - accuracy: 0.1429 - val_loss: 2.8064 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1421 - accuracy: 0.1429\n",
            "Epoch 6: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.1421 - accuracy: 0.1429 - val_loss: 3.9112 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0479 - accuracy: 0.1429\n",
            "Epoch 7: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0479 - accuracy: 0.1429 - val_loss: 5.1244 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2351 - accuracy: 0.1429\n",
            "Epoch 8: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.2351 - accuracy: 0.1429 - val_loss: 3.4302 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0340 - accuracy: 0.1429\n",
            "Epoch 9: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.0340 - accuracy: 0.1429 - val_loss: 3.2955 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.0230 - accuracy: 0.1429\n",
            "Epoch 10: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.0230 - accuracy: 0.1429 - val_loss: 3.4259 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.9888 - accuracy: 0.1429\n",
            "Epoch 11: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.9888 - accuracy: 0.1429 - val_loss: 4.6519 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7762 - accuracy: 0.1429\n",
            "Epoch 12: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7762 - accuracy: 0.1429 - val_loss: 6.2384 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.9324 - accuracy: 0.1429\n",
            "Epoch 13: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.9324 - accuracy: 0.1429 - val_loss: 6.3172 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7009 - accuracy: 0.2857\n",
            "Epoch 14: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7009 - accuracy: 0.2857 - val_loss: 3.9322 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7196 - accuracy: 0.2857\n",
            "Epoch 15: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7196 - accuracy: 0.2857 - val_loss: 3.8450 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7256 - accuracy: 0.2857\n",
            "Epoch 16: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7256 - accuracy: 0.2857 - val_loss: 3.9149 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.6764 - accuracy: 0.2857\n",
            "Epoch 17: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.6764 - accuracy: 0.2857 - val_loss: 4.1765 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5621 - accuracy: 0.4286\n",
            "Epoch 18: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5621 - accuracy: 0.4286 - val_loss: 4.8265 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5924 - accuracy: 0.4286\n",
            "Epoch 19: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5924 - accuracy: 0.4286 - val_loss: 6.2842 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5531 - accuracy: 0.2857\n",
            "Epoch 20: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5531 - accuracy: 0.2857 - val_loss: 6.4028 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5645 - accuracy: 0.2857\n",
            "Epoch 21: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5645 - accuracy: 0.2857 - val_loss: 5.9162 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4384 - accuracy: 0.2857\n",
            "Epoch 22: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.4384 - accuracy: 0.2857 - val_loss: 5.1259 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5339 - accuracy: 0.2857\n",
            "Epoch 23: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5339 - accuracy: 0.2857 - val_loss: 7.1734 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5334 - accuracy: 0.2857\n",
            "Epoch 24: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5334 - accuracy: 0.2857 - val_loss: 8.4099 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.7711 - accuracy: 0.2857\n",
            "Epoch 25: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7711 - accuracy: 0.2857 - val_loss: 7.5109 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2881 - accuracy: 0.2857\n",
            "Epoch 26: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.2881 - accuracy: 0.2857 - val_loss: 6.1921 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2823 - accuracy: 0.2857\n",
            "Epoch 27: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2823 - accuracy: 0.2857 - val_loss: 6.1317 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3505 - accuracy: 0.1429\n",
            "Epoch 28: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3505 - accuracy: 0.1429 - val_loss: 6.8697 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2540 - accuracy: 0.2857\n",
            "Epoch 29: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2540 - accuracy: 0.2857 - val_loss: 7.5481 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2612 - accuracy: 0.2857\n",
            "Epoch 30: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.2612 - accuracy: 0.2857 - val_loss: 7.7293 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2230 - accuracy: 0.4286\n",
            "Epoch 31: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2230 - accuracy: 0.4286 - val_loss: 9.2217 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1467 - accuracy: 0.4286\n",
            "Epoch 32: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.1467 - accuracy: 0.4286 - val_loss: 19.2281 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3289 - accuracy: 0.4286\n",
            "Epoch 33: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.3289 - accuracy: 0.4286 - val_loss: 13.9148 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0284 - accuracy: 0.5714\n",
            "Epoch 34: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.0284 - accuracy: 0.5714 - val_loss: 8.7042 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0925 - accuracy: 0.5714\n",
            "Epoch 35: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0925 - accuracy: 0.5714 - val_loss: 12.9671 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9562 - accuracy: 0.5714\n",
            "Epoch 36: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.9562 - accuracy: 0.5714 - val_loss: 15.8281 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0288 - accuracy: 0.4286\n",
            "Epoch 37: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0288 - accuracy: 0.4286 - val_loss: 11.8685 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9366 - accuracy: 0.5714\n",
            "Epoch 38: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9366 - accuracy: 0.5714 - val_loss: 9.2865 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8737 - accuracy: 0.5714\n",
            "Epoch 39: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8737 - accuracy: 0.5714 - val_loss: 8.9656 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8362 - accuracy: 0.5714\n",
            "Epoch 40: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.8362 - accuracy: 0.5714 - val_loss: 12.4847 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9828 - accuracy: 0.5714\n",
            "Epoch 41: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9828 - accuracy: 0.5714 - val_loss: 17.9173 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9367 - accuracy: 0.4286\n",
            "Epoch 42: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.9367 - accuracy: 0.4286 - val_loss: 24.0665 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7093 - accuracy: 0.7143\n",
            "Epoch 43: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7093 - accuracy: 0.7143 - val_loss: 30.7753 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8339 - accuracy: 0.5714\n",
            "Epoch 44: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.8339 - accuracy: 0.5714 - val_loss: 17.5257 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9016 - accuracy: 0.7143\n",
            "Epoch 45: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.9016 - accuracy: 0.7143 - val_loss: 9.8864 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2463 - accuracy: 0.5714\n",
            "Epoch 46: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.2463 - accuracy: 0.5714 - val_loss: 9.2497 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9660 - accuracy: 0.7143\n",
            "Epoch 47: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.9660 - accuracy: 0.7143 - val_loss: 10.3268 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9149 - accuracy: 0.7143\n",
            "Epoch 48: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9149 - accuracy: 0.7143 - val_loss: 12.2470 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0529 - accuracy: 0.4286\n",
            "Epoch 49: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.0529 - accuracy: 0.4286 - val_loss: 14.2783 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8803 - accuracy: 0.7143\n",
            "Epoch 50: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.8803 - accuracy: 0.7143 - val_loss: 17.4475 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0780 - accuracy: 0.5714\n",
            "Epoch 51: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.0780 - accuracy: 0.5714 - val_loss: 17.8505 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7271 - accuracy: 0.7143\n",
            "Epoch 52: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7271 - accuracy: 0.7143 - val_loss: 20.8435 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9835 - accuracy: 0.5714\n",
            "Epoch 53: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9835 - accuracy: 0.5714 - val_loss: 16.8836 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9262 - accuracy: 0.5714\n",
            "Epoch 54: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.9262 - accuracy: 0.5714 - val_loss: 20.5995 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1394 - accuracy: 0.4286\n",
            "Epoch 55: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1394 - accuracy: 0.4286 - val_loss: 9.5040 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1004 - accuracy: 0.5714\n",
            "Epoch 56: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.1004 - accuracy: 0.5714 - val_loss: 6.9474 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8814 - accuracy: 0.7143\n",
            "Epoch 57: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.8814 - accuracy: 0.7143 - val_loss: 10.8058 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.2020 - accuracy: 0.4286\n",
            "Epoch 58: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2020 - accuracy: 0.4286 - val_loss: 6.0726 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4556 - accuracy: 0.4286\n",
            "Epoch 59: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4556 - accuracy: 0.4286 - val_loss: 8.0307 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.5508 - accuracy: 0.1429\n",
            "Epoch 60: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5508 - accuracy: 0.1429 - val_loss: 4.4576 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2458 - accuracy: 0.4286\n",
            "Epoch 61: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.2458 - accuracy: 0.4286 - val_loss: 4.2884 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2409 - accuracy: 0.4286\n",
            "Epoch 62: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 3s 3s/step - loss: 1.2409 - accuracy: 0.4286 - val_loss: 4.4583 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3657 - accuracy: 0.4286\n",
            "Epoch 63: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3657 - accuracy: 0.4286 - val_loss: 6.5972 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3482 - accuracy: 0.4286\n",
            "Epoch 64: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3482 - accuracy: 0.4286 - val_loss: 9.0599 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4233 - accuracy: 0.4286\n",
            "Epoch 65: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4233 - accuracy: 0.4286 - val_loss: 10.8678 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1467 - accuracy: 0.4286\n",
            "Epoch 66: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.1467 - accuracy: 0.4286 - val_loss: 12.0216 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4552 - accuracy: 0.2857\n",
            "Epoch 67: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4552 - accuracy: 0.2857 - val_loss: 6.5406 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0548 - accuracy: 0.4286\n",
            "Epoch 68: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0548 - accuracy: 0.4286 - val_loss: 10.5657 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.1032 - accuracy: 0.2857\n",
            "Epoch 69: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1032 - accuracy: 0.2857 - val_loss: 25.5157 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 2.4569 - accuracy: 0.4286\n",
            "Epoch 70: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.4569 - accuracy: 0.4286 - val_loss: 18.9074 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.8858 - accuracy: 0.2857\n",
            "Epoch 71: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8858 - accuracy: 0.2857 - val_loss: 6.9058 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2163 - accuracy: 0.4286\n",
            "Epoch 72: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.2163 - accuracy: 0.4286 - val_loss: 4.3668 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1547 - accuracy: 0.5714\n",
            "Epoch 73: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.1547 - accuracy: 0.5714 - val_loss: 4.3980 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3544 - accuracy: 0.4286\n",
            "Epoch 74: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3544 - accuracy: 0.4286 - val_loss: 4.4615 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.5762 - accuracy: 0.4286\n",
            "Epoch 75: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5762 - accuracy: 0.4286 - val_loss: 4.4214 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.4567 - accuracy: 0.2857\n",
            "Epoch 76: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4567 - accuracy: 0.2857 - val_loss: 4.0261 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2852 - accuracy: 0.4286\n",
            "Epoch 77: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2852 - accuracy: 0.4286 - val_loss: 3.8493 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.2755 - accuracy: 0.4286\n",
            "Epoch 78: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.2755 - accuracy: 0.4286 - val_loss: 4.1800 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3022 - accuracy: 0.2857\n",
            "Epoch 79: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3022 - accuracy: 0.2857 - val_loss: 5.4454 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.3540 - accuracy: 0.2857\n",
            "Epoch 80: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3540 - accuracy: 0.2857 - val_loss: 4.3999 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1084 - accuracy: 0.4286\n",
            "Epoch 81: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.1084 - accuracy: 0.4286 - val_loss: 5.0668 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.1935 - accuracy: 0.5714\n",
            "Epoch 82: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.1935 - accuracy: 0.5714 - val_loss: 5.1700 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0325 - accuracy: 0.4286\n",
            "Epoch 83: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.0325 - accuracy: 0.4286 - val_loss: 6.5724 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0443 - accuracy: 0.4286\n",
            "Epoch 84: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.0443 - accuracy: 0.4286 - val_loss: 6.7537 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9902 - accuracy: 0.4286\n",
            "Epoch 85: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.9902 - accuracy: 0.4286 - val_loss: 6.0351 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0369 - accuracy: 0.4286\n",
            "Epoch 86: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.0369 - accuracy: 0.4286 - val_loss: 6.1700 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9822 - accuracy: 0.5714\n",
            "Epoch 87: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9822 - accuracy: 0.5714 - val_loss: 7.1397 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0271 - accuracy: 0.5714\n",
            "Epoch 88: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0271 - accuracy: 0.5714 - val_loss: 9.5235 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8709 - accuracy: 0.4286\n",
            "Epoch 89: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8709 - accuracy: 0.4286 - val_loss: 11.9552 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8551 - accuracy: 0.4286\n",
            "Epoch 90: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8551 - accuracy: 0.4286 - val_loss: 13.3023 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8177 - accuracy: 0.5714\n",
            "Epoch 91: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8177 - accuracy: 0.5714 - val_loss: 14.4968 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9753 - accuracy: 0.4286\n",
            "Epoch 92: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.9753 - accuracy: 0.4286 - val_loss: 14.7340 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7977 - accuracy: 0.5714\n",
            "Epoch 93: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7977 - accuracy: 0.5714 - val_loss: 15.8744 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7554 - accuracy: 0.5714\n",
            "Epoch 94: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7554 - accuracy: 0.5714 - val_loss: 17.6921 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7329 - accuracy: 0.5714\n",
            "Epoch 95: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7329 - accuracy: 0.5714 - val_loss: 18.2687 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6979 - accuracy: 0.7143\n",
            "Epoch 96: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6979 - accuracy: 0.7143 - val_loss: 22.9237 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7414 - accuracy: 0.4286\n",
            "Epoch 97: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7414 - accuracy: 0.4286 - val_loss: 26.7798 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8223 - accuracy: 0.5714\n",
            "Epoch 98: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.8223 - accuracy: 0.5714 - val_loss: 35.2902 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7414 - accuracy: 0.4286\n",
            "Epoch 99: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7414 - accuracy: 0.4286 - val_loss: 31.2069 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7621 - accuracy: 0.5714\n",
            "Epoch 100: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7621 - accuracy: 0.5714 - val_loss: 24.2872 - val_accuracy: 0.0000e+00\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6111 - accuracy: 0.7143\n",
            "Epoch 101: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6111 - accuracy: 0.7143 - val_loss: 26.0070 - val_accuracy: 0.0000e+00\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 1.0089 - accuracy: 0.5714\n",
            "Epoch 102: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.0089 - accuracy: 0.5714 - val_loss: 25.2101 - val_accuracy: 0.0000e+00\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7055 - accuracy: 0.5714\n",
            "Epoch 103: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7055 - accuracy: 0.5714 - val_loss: 19.9635 - val_accuracy: 0.0000e+00\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7796 - accuracy: 0.4286\n",
            "Epoch 104: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7796 - accuracy: 0.4286 - val_loss: 15.0547 - val_accuracy: 0.0000e+00\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7609 - accuracy: 0.5714\n",
            "Epoch 105: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7609 - accuracy: 0.5714 - val_loss: 11.0712 - val_accuracy: 0.0000e+00\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7516 - accuracy: 0.7143\n",
            "Epoch 106: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7516 - accuracy: 0.7143 - val_loss: 9.7952 - val_accuracy: 0.0000e+00\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7295 - accuracy: 0.7143\n",
            "Epoch 107: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7295 - accuracy: 0.7143 - val_loss: 10.7017 - val_accuracy: 0.0000e+00\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7500 - accuracy: 0.7143\n",
            "Epoch 108: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7500 - accuracy: 0.7143 - val_loss: 13.7213 - val_accuracy: 0.0000e+00\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.7143\n",
            "Epoch 109: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6785 - accuracy: 0.7143 - val_loss: 17.9953 - val_accuracy: 0.0000e+00\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7122 - accuracy: 0.5714\n",
            "Epoch 110: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7122 - accuracy: 0.5714 - val_loss: 20.7515 - val_accuracy: 0.0000e+00\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8291 - accuracy: 0.4286\n",
            "Epoch 111: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.8291 - accuracy: 0.4286 - val_loss: 19.7143 - val_accuracy: 0.0000e+00\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7039 - accuracy: 0.7143\n",
            "Epoch 112: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7039 - accuracy: 0.7143 - val_loss: 20.8983 - val_accuracy: 0.0000e+00\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6336 - accuracy: 0.5714\n",
            "Epoch 113: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6336 - accuracy: 0.5714 - val_loss: 22.1766 - val_accuracy: 0.0000e+00\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7079 - accuracy: 0.5714\n",
            "Epoch 114: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7079 - accuracy: 0.5714 - val_loss: 21.1236 - val_accuracy: 0.0000e+00\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6418 - accuracy: 0.7143\n",
            "Epoch 115: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6418 - accuracy: 0.7143 - val_loss: 19.0458 - val_accuracy: 0.0000e+00\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8991 - accuracy: 0.5714\n",
            "Epoch 116: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.8991 - accuracy: 0.5714 - val_loss: 21.4311 - val_accuracy: 0.0000e+00\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7944 - accuracy: 0.5714\n",
            "Epoch 117: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7944 - accuracy: 0.5714 - val_loss: 16.7006 - val_accuracy: 0.0000e+00\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5951 - accuracy: 0.7143\n",
            "Epoch 118: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5951 - accuracy: 0.7143 - val_loss: 14.1958 - val_accuracy: 0.0000e+00\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6479 - accuracy: 0.7143\n",
            "Epoch 119: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6479 - accuracy: 0.7143 - val_loss: 12.9427 - val_accuracy: 0.0000e+00\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.7143\n",
            "Epoch 120: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6949 - accuracy: 0.7143 - val_loss: 12.3216 - val_accuracy: 0.0000e+00\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8037 - accuracy: 0.7143\n",
            "Epoch 121: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.8037 - accuracy: 0.7143 - val_loss: 12.1644 - val_accuracy: 0.0000e+00\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7515 - accuracy: 0.5714\n",
            "Epoch 122: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7515 - accuracy: 0.5714 - val_loss: 11.8327 - val_accuracy: 0.0000e+00\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7524 - accuracy: 0.7143\n",
            "Epoch 123: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7524 - accuracy: 0.7143 - val_loss: 11.6315 - val_accuracy: 0.0000e+00\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8908 - accuracy: 0.5714\n",
            "Epoch 124: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.8908 - accuracy: 0.5714 - val_loss: 11.0952 - val_accuracy: 0.0000e+00\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7107 - accuracy: 0.5714\n",
            "Epoch 125: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7107 - accuracy: 0.5714 - val_loss: 11.2357 - val_accuracy: 0.0000e+00\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6991 - accuracy: 0.5714\n",
            "Epoch 126: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6991 - accuracy: 0.5714 - val_loss: 11.8281 - val_accuracy: 0.0000e+00\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6732 - accuracy: 0.5714\n",
            "Epoch 127: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6732 - accuracy: 0.5714 - val_loss: 11.9709 - val_accuracy: 0.0000e+00\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7547 - accuracy: 0.5714\n",
            "Epoch 128: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7547 - accuracy: 0.5714 - val_loss: 14.0243 - val_accuracy: 0.0000e+00\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7158 - accuracy: 0.5714\n",
            "Epoch 129: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7158 - accuracy: 0.5714 - val_loss: 14.5516 - val_accuracy: 0.0000e+00\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7461 - accuracy: 0.5714\n",
            "Epoch 130: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.7461 - accuracy: 0.5714 - val_loss: 14.0091 - val_accuracy: 0.0000e+00\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6821 - accuracy: 0.4286\n",
            "Epoch 131: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6821 - accuracy: 0.4286 - val_loss: 12.8910 - val_accuracy: 0.0000e+00\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6149 - accuracy: 0.7143\n",
            "Epoch 132: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6149 - accuracy: 0.7143 - val_loss: 12.1735 - val_accuracy: 0.0000e+00\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6065 - accuracy: 0.7143\n",
            "Epoch 133: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6065 - accuracy: 0.7143 - val_loss: 11.9827 - val_accuracy: 0.0000e+00\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6607 - accuracy: 0.7143\n",
            "Epoch 134: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6607 - accuracy: 0.7143 - val_loss: 12.4603 - val_accuracy: 0.0000e+00\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6763 - accuracy: 0.7143\n",
            "Epoch 135: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6763 - accuracy: 0.7143 - val_loss: 13.2244 - val_accuracy: 0.0000e+00\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6122 - accuracy: 0.7143\n",
            "Epoch 136: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6122 - accuracy: 0.7143 - val_loss: 14.0831 - val_accuracy: 0.0000e+00\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5704 - accuracy: 0.7143\n",
            "Epoch 137: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5704 - accuracy: 0.7143 - val_loss: 15.9747 - val_accuracy: 0.0000e+00\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5744 - accuracy: 0.7143\n",
            "Epoch 138: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5744 - accuracy: 0.7143 - val_loss: 17.9321 - val_accuracy: 0.0000e+00\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5371 - accuracy: 0.7143\n",
            "Epoch 139: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5371 - accuracy: 0.7143 - val_loss: 19.5266 - val_accuracy: 0.0000e+00\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5972 - accuracy: 0.5714\n",
            "Epoch 140: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5972 - accuracy: 0.5714 - val_loss: 19.7874 - val_accuracy: 0.0000e+00\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4903 - accuracy: 0.7143\n",
            "Epoch 141: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4903 - accuracy: 0.7143 - val_loss: 20.2562 - val_accuracy: 0.0000e+00\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4580 - accuracy: 0.7143\n",
            "Epoch 142: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4580 - accuracy: 0.7143 - val_loss: 21.0306 - val_accuracy: 0.0000e+00\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.8955 - accuracy: 0.5714\n",
            "Epoch 143: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.8955 - accuracy: 0.5714 - val_loss: 22.9648 - val_accuracy: 0.0000e+00\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4741 - accuracy: 0.7143\n",
            "Epoch 144: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4741 - accuracy: 0.7143 - val_loss: 25.3956 - val_accuracy: 0.0000e+00\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4783 - accuracy: 0.7143\n",
            "Epoch 145: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4783 - accuracy: 0.7143 - val_loss: 27.7583 - val_accuracy: 0.0000e+00\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.9310 - accuracy: 0.5714\n",
            "Epoch 146: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.9310 - accuracy: 0.5714 - val_loss: 23.2899 - val_accuracy: 0.0000e+00\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6996 - accuracy: 0.5714\n",
            "Epoch 147: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6996 - accuracy: 0.5714 - val_loss: 20.7541 - val_accuracy: 0.0000e+00\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.7143\n",
            "Epoch 148: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4743 - accuracy: 0.7143 - val_loss: 25.4503 - val_accuracy: 0.0000e+00\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5799 - accuracy: 0.5714\n",
            "Epoch 149: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5799 - accuracy: 0.5714 - val_loss: 18.4979 - val_accuracy: 0.0000e+00\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4328 - accuracy: 0.7143\n",
            "Epoch 150: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4328 - accuracy: 0.7143 - val_loss: 14.8236 - val_accuracy: 0.0000e+00\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4920 - accuracy: 0.7143\n",
            "Epoch 151: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4920 - accuracy: 0.7143 - val_loss: 13.8552 - val_accuracy: 0.0000e+00\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5731 - accuracy: 0.8571\n",
            "Epoch 152: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5731 - accuracy: 0.8571 - val_loss: 13.8395 - val_accuracy: 0.0000e+00\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.7340 - accuracy: 0.8571\n",
            "Epoch 153: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.7340 - accuracy: 0.8571 - val_loss: 13.2907 - val_accuracy: 0.0000e+00\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5793 - accuracy: 1.0000\n",
            "Epoch 154: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.5793 - accuracy: 1.0000 - val_loss: 13.3020 - val_accuracy: 0.0000e+00\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5454 - accuracy: 1.0000\n",
            "Epoch 155: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.5454 - accuracy: 1.0000 - val_loss: 13.7385 - val_accuracy: 0.0000e+00\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4900 - accuracy: 0.8571\n",
            "Epoch 156: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4900 - accuracy: 0.8571 - val_loss: 14.7498 - val_accuracy: 0.0000e+00\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.6085 - accuracy: 0.5714\n",
            "Epoch 157: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.6085 - accuracy: 0.5714 - val_loss: 16.6421 - val_accuracy: 0.0000e+00\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4320 - accuracy: 0.7143\n",
            "Epoch 158: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4320 - accuracy: 0.7143 - val_loss: 19.4475 - val_accuracy: 0.0000e+00\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4321 - accuracy: 0.7143\n",
            "Epoch 159: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4321 - accuracy: 0.7143 - val_loss: 22.1648 - val_accuracy: 0.0000e+00\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4173 - accuracy: 0.7143\n",
            "Epoch 160: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4173 - accuracy: 0.7143 - val_loss: 24.7599 - val_accuracy: 0.0000e+00\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4852 - accuracy: 0.7143\n",
            "Epoch 161: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4852 - accuracy: 0.7143 - val_loss: 26.4407 - val_accuracy: 0.0000e+00\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4380 - accuracy: 0.7143\n",
            "Epoch 162: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4380 - accuracy: 0.7143 - val_loss: 26.5530 - val_accuracy: 0.0000e+00\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3808 - accuracy: 0.7143\n",
            "Epoch 163: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3808 - accuracy: 0.7143 - val_loss: 26.8764 - val_accuracy: 0.0000e+00\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3563 - accuracy: 0.7143\n",
            "Epoch 164: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3563 - accuracy: 0.7143 - val_loss: 27.4371 - val_accuracy: 0.0000e+00\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4377 - accuracy: 0.7143\n",
            "Epoch 165: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4377 - accuracy: 0.7143 - val_loss: 27.1899 - val_accuracy: 0.0000e+00\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3634 - accuracy: 0.7143\n",
            "Epoch 166: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3634 - accuracy: 0.7143 - val_loss: 27.3816 - val_accuracy: 0.0000e+00\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.5426 - accuracy: 0.5714\n",
            "Epoch 167: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5426 - accuracy: 0.5714 - val_loss: 26.6002 - val_accuracy: 0.0000e+00\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3584 - accuracy: 0.7143\n",
            "Epoch 168: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3584 - accuracy: 0.7143 - val_loss: 26.5546 - val_accuracy: 0.0000e+00\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.7143\n",
            "Epoch 169: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3445 - accuracy: 0.7143 - val_loss: 27.0062 - val_accuracy: 0.0000e+00\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.7143\n",
            "Epoch 170: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3345 - accuracy: 0.7143 - val_loss: 27.5153 - val_accuracy: 0.0000e+00\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3447 - accuracy: 0.7143\n",
            "Epoch 171: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3447 - accuracy: 0.7143 - val_loss: 28.4806 - val_accuracy: 0.0000e+00\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.7143\n",
            "Epoch 172: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3323 - accuracy: 0.7143 - val_loss: 29.5467 - val_accuracy: 0.0000e+00\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3342 - accuracy: 0.7143\n",
            "Epoch 173: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3342 - accuracy: 0.7143 - val_loss: 30.5538 - val_accuracy: 0.0000e+00\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3417 - accuracy: 0.7143\n",
            "Epoch 174: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3417 - accuracy: 0.7143 - val_loss: 31.5700 - val_accuracy: 0.0000e+00\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3862 - accuracy: 0.7143\n",
            "Epoch 175: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3862 - accuracy: 0.7143 - val_loss: 32.3780 - val_accuracy: 0.0000e+00\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3418 - accuracy: 0.7143\n",
            "Epoch 176: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3418 - accuracy: 0.7143 - val_loss: 33.1885 - val_accuracy: 0.0000e+00\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3445 - accuracy: 0.7143\n",
            "Epoch 177: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3445 - accuracy: 0.7143 - val_loss: 34.0911 - val_accuracy: 0.0000e+00\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3456 - accuracy: 0.8571\n",
            "Epoch 178: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3456 - accuracy: 0.8571 - val_loss: 35.4689 - val_accuracy: 0.0000e+00\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3630 - accuracy: 0.8571\n",
            "Epoch 179: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3630 - accuracy: 0.8571 - val_loss: 36.9478 - val_accuracy: 0.0000e+00\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.8571\n",
            "Epoch 180: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3495 - accuracy: 0.8571 - val_loss: 39.3844 - val_accuracy: 0.0000e+00\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3187 - accuracy: 1.0000\n",
            "Epoch 181: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3187 - accuracy: 1.0000 - val_loss: 43.3053 - val_accuracy: 0.0000e+00\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3171 - accuracy: 0.8571\n",
            "Epoch 182: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3171 - accuracy: 0.8571 - val_loss: 46.1146 - val_accuracy: 0.0000e+00\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2761 - accuracy: 1.0000\n",
            "Epoch 183: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.2761 - accuracy: 1.0000 - val_loss: 49.5820 - val_accuracy: 0.0000e+00\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.8571\n",
            "Epoch 184: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.2757 - accuracy: 0.8571 - val_loss: 50.2872 - val_accuracy: 0.0000e+00\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3533 - accuracy: 0.7143\n",
            "Epoch 185: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3533 - accuracy: 0.7143 - val_loss: 53.0344 - val_accuracy: 0.0000e+00\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4019 - accuracy: 0.7143\n",
            "Epoch 186: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4019 - accuracy: 0.7143 - val_loss: 52.1894 - val_accuracy: 0.0000e+00\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.7143\n",
            "Epoch 187: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3323 - accuracy: 0.7143 - val_loss: 48.1913 - val_accuracy: 0.0000e+00\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.4366 - accuracy: 0.7143\n",
            "Epoch 188: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.4366 - accuracy: 0.7143 - val_loss: 43.3777 - val_accuracy: 0.0000e+00\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3213 - accuracy: 0.8571\n",
            "Epoch 189: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3213 - accuracy: 0.8571 - val_loss: 44.4708 - val_accuracy: 0.0000e+00\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3370 - accuracy: 0.8571\n",
            "Epoch 190: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3370 - accuracy: 0.8571 - val_loss: 44.7708 - val_accuracy: 0.0000e+00\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3514 - accuracy: 0.8571\n",
            "Epoch 191: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3514 - accuracy: 0.8571 - val_loss: 48.7004 - val_accuracy: 0.0000e+00\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3332 - accuracy: 0.8571\n",
            "Epoch 192: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3332 - accuracy: 0.8571 - val_loss: 49.9822 - val_accuracy: 0.0000e+00\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3422 - accuracy: 0.8571\n",
            "Epoch 193: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3422 - accuracy: 0.8571 - val_loss: 50.5139 - val_accuracy: 0.0000e+00\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.8571\n",
            "Epoch 194: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3345 - accuracy: 0.8571 - val_loss: 50.9264 - val_accuracy: 0.0000e+00\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3253 - accuracy: 0.8571\n",
            "Epoch 195: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3253 - accuracy: 0.8571 - val_loss: 51.0485 - val_accuracy: 0.0000e+00\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3217 - accuracy: 0.8571\n",
            "Epoch 196: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3217 - accuracy: 0.8571 - val_loss: 50.8773 - val_accuracy: 0.0000e+00\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3231 - accuracy: 0.8571\n",
            "Epoch 197: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3231 - accuracy: 0.8571 - val_loss: 50.8823 - val_accuracy: 0.0000e+00\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.8571\n",
            "Epoch 198: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.3221 - accuracy: 0.8571 - val_loss: 50.8581 - val_accuracy: 0.0000e+00\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3246 - accuracy: 0.8571\n",
            "Epoch 199: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3246 - accuracy: 0.8571 - val_loss: 50.9666 - val_accuracy: 0.0000e+00\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.3700 - accuracy: 0.8571\n",
            "Epoch 200: val_loss did not improve from 2.51250\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3700 - accuracy: 0.8571 - val_loss: 49.6098 - val_accuracy: 0.0000e+00\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 2.3268 - accuracy: 0.1000\n",
            "Test accuracy: 10.0%\n"
          ]
        }
      ],
      "source": [
        "# Created function for compliling model, Model intilization..\n",
        "def sequence_model():  # Using  LSTM Model . Compiling model and return output\n",
        "    vocab = label_processor_phases.get_vocabulary()\n",
        "\n",
        "    frame_features_as_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES)) # input to RNN \n",
        "    mask_as_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\") # Input to RNN\n",
        "\n",
        "    x = keras.layers.LSTM(100, activation='relu',return_sequences=True)(\n",
        "        frame_features_as_input, mask=mask_as_input\n",
        "    )\n",
        "    x = keras.layers.LSTM(50, activation='relu')(x) # Layers\n",
        "    x = keras.layers.Dropout(0.1)(x)\n",
        "    x = keras.layers.Dense(2048, activation=\"relu\")(x)\n",
        "    x = keras.layers.Dense(1024, activation=\"relu\")(x)\n",
        "    x = keras.layers.Dense(512, activation=\"relu\")(x)\n",
        "\n",
        "    x = keras.layers.Dense(126, activation=\"relu\")(x)\n",
        "    x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
        "    x = keras.layers.Dense(16, activation=\"relu\")(x)\n",
        "    output_value = keras.layers.Dense(len(vocab), activation=\"softmax\")(x) # final layer\n",
        "\n",
        "    lstm_model = keras.Model([frame_features_as_input, mask_as_input], output_value)\n",
        "\n",
        "    lstm_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]  # this cost function we are using \n",
        "    )\n",
        "    return lstm_model\n",
        "\n",
        "\n",
        "# This function for runing the model, OR performing the traainingg\n",
        "def run_our_experiment():\n",
        "    file_path = \"/tmp/video_classifier\" # Under temp folder , model will be stored\n",
        "    checkpoint_ = keras.callbacks.ModelCheckpoint(\n",
        "        file_path, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "    ## Model weights are saved at the end of every epoch, if it's the best seen\n",
        "# so far\n",
        "#https://keras.io/api/callbacks/model_checkpoint/#:~:text=ModelCheckpoint%20callback%20is%20used%20in,training%20from%20the%20state%20saved.\n",
        "    seq_model = sequence_model() # Calling  Function which we already defined\n",
        "    history = seq_model.fit( # fitting model, \n",
        "        [train_data[0], train_data[1]], #  Training Data ( frames)\n",
        "        train_labels, # Training labels ( Phases)\n",
        "        validation_split=0.3, # Validation split\n",
        "        epochs=EPOCHS, # Epochs\n",
        "        callbacks=[checkpoint_],\n",
        "    )\n",
        "\n",
        "    seq_model.load_weights(file_path)\n",
        "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history, seq_model\n",
        "\n",
        "\n",
        "history, sequence_model = run_our_experiment() #return history to plot graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "kRERHQKK3vm1",
        "outputId": "7649aa91-e995-4c2f-8534-5e7fb8a1d794"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#Visualize Training\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history[\"val_\"+string])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string,\"val_\"+string])\n",
        "    plt.show()\n",
        "plot_graphs(history,'accuracy')\n",
        "plot_graphs(history,'loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "egaFPyKl5Nc1",
        "outputId": "0f783954-5ecf-4c1e-8aa5-849dd50d4114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 763ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "class_labels = [i for i in range(10)] #10 classes\n",
        "\n",
        "from sklearn import metrics\n",
        "#Confution Matrix\n",
        "Y_pred = sequence_model.predict([test_data[0], test_data[1]]) #predict result\n",
        "y_pred = np.argmax(Y_pred, axis=1) #get class\n",
        "cm = metrics.confusion_matrix(test_labels, y_pred)\n",
        "\n",
        "# class_labels = list(test_generator.class_indices.keys()) \n",
        "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels) #plot confusion matrix\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "mrM-HaxVFX-H",
        "outputId": "e76494d1-e1a4-4857-aa53-48a0eb6fe980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test video path: /content/drive/MyDrive/Dataset/dataset_4/1/video.avi\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "  1: 10.06%\n",
            "  6: 10.05%\n",
            "  9: 10.04%\n",
            "  10: 10.04%\n",
            "  8: 10.01%\n",
            "  5: 10.00%\n",
            "  7:  9.99%\n",
            "  3:  9.99%\n",
            "  4:  9.92%\n",
            "  2:  9.91%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-12cf03aa285e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test video path: {test_video}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mtest_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_prediction_for_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_video\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mto_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-12cf03aa285e>\u001b[0m in \u001b[0;36mto_gif\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mconverted_images_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mimageio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"animation.gif\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverted_images_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# at rate of 10 fbs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"animation.gif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mmimwrite\u001b[0;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;31m# be a generator. The damage is done, but we want to error when it happens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwritten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Zero images were written.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;31m# Return a result if there is any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Zero images were written."
          ]
        }
      ],
      "source": [
        "# Preparing for prediction\n",
        " #This function is responsible to prepare video which algorithm will accept for predtion\n",
        "# If the number of frames are less, we are using this Max_Seq \n",
        "#OR padding\n",
        "def prepare_single_video_for_prediction(frames):\n",
        "    frames_1 = frames[None, ...]\n",
        "    frame_mask_1 = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "    frame_features_values = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "\n",
        "    for i, batch in enumerate(frames_1):\n",
        "        video_len = batch.shape[0]\n",
        "        len = min(50, video_len)\n",
        "        for j in range(len):\n",
        "            frame_features_values[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "        frame_mask_1[i, :len] = 1  \n",
        "\n",
        "    return frame_features_values, frame_mask_1\n",
        "\n",
        "\n",
        "def sequence_prediction_for_frames(path):\n",
        "    vocab = label_processor_phases.get_vocabulary()\n",
        "\n",
        "    frames_all = load_video(os.path.join(\"test\", path))\n",
        "    frame_features_value, frame_mask_1 = prepare_single_video_for_prediction(frames_all)\n",
        "    probabilities = sequence_model.predict([frame_features_value, frame_mask_1])[0] # Prediction\n",
        "\n",
        "    for i in np.argsort(probabilities)[::-1]:\n",
        "        print(f\"  {vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
        "    return frames_all\n",
        "\n",
        "def to_gif(images):\n",
        "    converted_images_value = images.astype(np.uint8)\n",
        "    imageio.mimsave(\"animation.gif\", converted_images_value, fps=10) # at rate of 10 fbs.\n",
        "    return embed.embed_file(\"animation.gif\")\n",
        "\n",
        "\n",
        "test_video = '/content/drive/MyDrive/Dataset/dataset_4/1/video.avi' # this video we want to know related phases.\n",
        "print(f\"Test video path: {test_video}\")\n",
        "test_frames = sequence_prediction_for_frames(test_video)\n",
        "to_gif(test_frames[:MAX_SEQ_LENGTH])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxoL8xXSFYDL"
      },
      "outputs": [],
      "source": [
        "# References\n",
        "#https://keras.io/examples/vision/video_classification/\n",
        "#https://www.youtube.com/watch?v=ezjnySXqdTo&ab_channel=CodeWithAarohi\n",
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3hQN9lWCu-y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy6KrTYZCvAd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
